{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16170716",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Unsupervised machine learning aims to learn patterns from data without predefined labels. Specifically, the goal is to learn a function $f(x)$ from the dataset $D = \\{\\mathbf{x}_i\\}$ by optimizing an objective function $g(D, f)$, or by simply partitioning the dataset $D$. This chapter provides an overview of clustering methods, which are a core part of unsupervised machine learning.\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Clustering is a technique used to partition data into distinct groups, where data points in the same group share similar characteristics. Clustering can be broadly categorized into **hard clustering** and **soft (fuzzy) clustering**.\n",
    "\n",
    "### Hard Clustering\n",
    "In hard clustering, each data point is assigned to a single cluster. This means that each data point belongs exclusively to one group.\n",
    "\n",
    "### Soft (Fuzzy) Clustering\n",
    "In soft clustering, a data point can belong to multiple clusters with different probabilities. This allows for a more nuanced assignment, where data points can have partial membership across clusters.\n",
    "\n",
    "## k-Means Clustering\n",
    "\n",
    "One of the most common clustering algorithms is **k-Means**. It is a hard clustering algorithm that aims to partition the dataset into $k$ clusters by iteratively refining the cluster assignments.\n",
    "\n",
    "### Algorithm Overview\n",
    "- **Cluster Centers**: The cluster center $\\mathbf{m}_i$ is defined as the arithmetic mean of all the data points belonging to the cluster.\n",
    "- **Distance Metric**: Typically, the distance metric used is Euclidean distance, defined as $||\\mathbf{x}_p - \\mathbf{m}_i||^2$.\n",
    "- **Cluster Assignment**: Each data point is assigned to the nearest cluster center:\n",
    "  \n",
    "  $$S_i^{(t)} = \\{\\mathbf{x}_p: ||\\mathbf{x}_p - \\mathbf{m}_i^{(t)}||^2 \\le ||\\mathbf{x}_p - \\mathbf{m}_j^{(t)}||^2 \\; \\forall j \\}$$\n",
    "  \n",
    "- **Recalculation of Cluster Centers**: The cluster centers are recalculated based on the new assignments:\n",
    "  \n",
    "  $$\\mathbf{m}_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum_{\\mathbf{x}_j \\in S_i^{(t)}} \\mathbf{x}_j$$\n",
    "\n",
    "These steps are repeated until the cluster centers converge.\n",
    "\n",
    "### Illustrations of k-Means\n",
    "\n",
    "To better understand how the k-Means algorithm works, let's consider the following visualization steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a313a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "X, y_true = make_blobs(n_samples=300, centers=5, cluster_std=0.60, random_state=1)\n",
    "\n",
    "# Function to perform one iteration of the k-Means EM step and plot in specified axes\n",
    "def plot_kmeans_step(ax, X, centers, step_title):\n",
    "    labels = pairwise_distances_argmin(X, centers)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='plasma')\n",
    "    sns.scatterplot(x=centers[:, 0], y=centers[:, 1], color='black', s=200, alpha=0.5, ax=ax)\n",
    "    ax.set_title(step_title)\n",
    "    return labels\n",
    "\n",
    "# Initialize the plot with a 4x2 grid (4 steps per column for E and M steps)\n",
    "fig, axes = plt.subplots(4, 2, figsize=(12, 16))\n",
    "fig.tight_layout(pad=5)\n",
    "\n",
    "# Step 1: Initial Random Cluster Centers\n",
    "rng = np.random.RandomState(1)\n",
    "i = rng.permutation(X.shape[0])[:5]\n",
    "centers = X[i]\n",
    "plot_kmeans_step(axes[0, 1], X, centers, \"Initial Random Cluster Centers\")\n",
    "\n",
    "# Step 2: First E-Step (assign points to the nearest cluster center)\n",
    "labels = plot_kmeans_step(axes[1, 0], X, centers, \"First E-Step: Assign Points to Nearest Cluster\")\n",
    "\n",
    "# Step 3: First M-Step (recalculate cluster centers)\n",
    "centers = np.array([X[labels == i].mean(0) for i in range(5)])\n",
    "plot_kmeans_step(axes[1, 1], X, centers, \"First M-Step: Recalculate Cluster Centers\")\n",
    "\n",
    "# Step 4: Second E-Step (assign points to the nearest cluster center)\n",
    "labels = plot_kmeans_step(axes[2, 0], X, centers, \"Second E-Step: Assign Points to Nearest Cluster\")\n",
    "\n",
    "# Step 5: Second M-Step (recalculate cluster centers)\n",
    "centers = np.array([X[labels == i].mean(0) for i in range(5)])\n",
    "plot_kmeans_step(axes[2, 1], X, centers, \"Second M-Step: Recalculate Cluster Centers\")\n",
    "\n",
    "# Step 6: Third E-Step (assign points to the nearest cluster center)\n",
    "labels = plot_kmeans_step(axes[3, 0], X, centers, \"Third E-Step: Assign Points to Nearest Cluster\")\n",
    "\n",
    "# Step 7: Third M-Step (recalculate cluster centers)\n",
    "centers = np.array([X[labels == i].mean(0) for i in range(5)])\n",
    "plot_kmeans_step(axes[3, 1], X, centers, \"Third M-Step: Recalculate Cluster Centers\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8f37a",
   "metadata": {},
   "source": [
    "The algorithm automatically assigns the points to clusters, and we can see that it closely matches what we would expect by visual inspection.\n",
    "\n",
    "### Drawbacks of k-Means\n",
    "\n",
    "The k-Means algorithm is simple and effective, but it has some drawbacks:\n",
    "\n",
    "1. **Initialization Sensitivity**: The algorithm's result can be highly dependent on the initial choice of cluster centers. Poor initialization may lead to suboptimal clustering, as k-Means may converge to a local minimum. For example, using a different random seed can lead to different cluster assignments.\n",
    "2. **Predefined Number of Clusters**: k-Means requires specifying the number of clusters beforehand. For example, if we set the number of clusters to three instead of five, the algorithm will still proceed, but the result may not be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c631a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "labels = KMeans(3, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='plasma');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e8193",
   "metadata": {},
   "source": [
    "3. **Linear Cluster Boundaries**: The k-Means algorithm assumes that clusters are spherical and separated by linear boundaries. It struggles with complex geometries. Consider the following dataset with two crescent-shaped clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(200, noise=.05, random_state=42)\n",
    "labels = KMeans(2, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='plasma');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff5860",
   "metadata": {},
   "source": [
    "4. **Differences in euclidian size**: K-Means assumes that the cluster sizes, in terms of euclidian distance to its borders, are fairly similar for all clusters.\n",
    "Here is an example of three \"Mikey Mouse\" shaped cludsters, where k-means seems to have a hard time allocating cluster boarders correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88811b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the blobs\n",
    "n_samples = [600, 100, 100]  \n",
    "centers = [(0, 0), (3, 3), (-3, 3)]  # Center coordinates\n",
    "cluster_std = [2., 0.5, 0.5]  # Standard deviations for each blob\n",
    "\n",
    "# Generate blobs\n",
    "X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=1)\n",
    "\n",
    "labels = KMeans(3, random_state=0).fit_predict(X)\n",
    "# Define markers for the original labels\n",
    "markers = ['o', 's', 'D']  # Circle, square, and diamond markers for each blob\n",
    "\n",
    "# Plot with original labels using different marker types\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(3):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], c=[i]*len(X[y == i]), \n",
    "                label=f\"Cluster {i+1} (Original)\", marker=markers[i], edgecolor='k', s=50, cmap='plasma')\n",
    "\n",
    "# Overlay KMeans labels as color-coded dots without specific marker shapes\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=20, cmap='plasma', alpha=0.4)\n",
    "plt.title(\"Generated Blobs with Original Labels and KMeans Clustering\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d3b44",
   "metadata": {},
   "source": [
    "## Multivariate Normal Distribution\n",
    "\n",
    "A **multivariate normal distribution**, also known as a **multinormal distribution**, is a generalization of the one-dimensional normal distribution to multiple dimensions.\n",
    "\n",
    "### Definition\n",
    "A multivariate normal distribution is characterized by a vector of mean values ($\\boldsymbol{\\mu}$) and a covariance matrix ($\\boldsymbol{\\Sigma}$). It describes the joint distribution of a set of random variables, each of which has a univariate normal distribution, but with the potential for covariance between them, allowing for dependencies. Mathematically, if $\\mathbf{X}$ is a random vector $(X_1, X_2, \\dots, X_n)$ following a multivariate normal distribution, its probability density function is given by:\n",
    "\n",
    "$$\\mathcal{N}( \\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^k |\\boldsymbol{\\Sigma}|}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}$ is a real $k$-dimensional vector,\n",
    "- $\\boldsymbol{\\mu}$ is the mean vector,\n",
    "- $\\boldsymbol{\\Sigma}$ is the $k \\times k$ covariance matrix,\n",
    "- $k$ is the number of dimensions (variables),\n",
    "- $|\\boldsymbol{\\Sigma}|$ is the determinant of the covariance matrix.\n",
    "\n",
    "The multivariate normal distribution is fundamental to Gaussian Mixture Models and provides a natural way to model the clusters in high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6a1f0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Parameters for the bivariate normal distribution\n",
    "mean = [0, 0]  # Mean vector\n",
    "cov = [[1, 0.5], [0.5, 1]]  # Covariance matrix\n",
    "\n",
    "# Generate grid points for plotting\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "# Calculate the probability density function\n",
    "rv = multivariate_normal(mean, cov)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "# Create a figure with two subplots: a 3D surface and a 2D contour plot\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 3D Surface plot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='k', alpha=0.7)\n",
    "ax.set_title(\"3D Surface Plot of Bivariate Normal Distribution\")\n",
    "ax.set_xlabel(\"X-axis\")\n",
    "ax.set_ylabel(\"Y-axis\")\n",
    "ax.set_zlabel(\"Density\")\n",
    "\n",
    "# 2D Contour plot\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "contour = ax2.contourf(X, Y, Z, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax2, label=\"Density\")\n",
    "ax2.set_title(\"2D Contour Plot of Bivariate Normal Distribution\")\n",
    "ax2.set_xlabel(\"X-axis\")\n",
    "ax2.set_ylabel(\"Y-axis\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427aeed",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models (GMM)\n",
    "\n",
    "**Gaussian Mixture Models (GMMs)** provide a probabilistic approach to clustering and are an example of soft clustering. GMMs assume that the data is generated from a mixture of several Gaussian distributions, each representing a cluster. Unlike k-Means, GMM provides a soft clustering where each point is assigned a probability of belonging to each cluster.\n",
    "\n",
    "\n",
    "### Illustrations of GMM\n",
    "\n",
    "To understand GMM better, let's consider the following visualizations.\n",
    "\n",
    "1. Here is a set of random datapoints generated from distributions where we introduced co-variation between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Function to generate diagonal blobs with custom shapes\n",
    "def generate_diagonal_blobs(means, covariances, n_samples):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, (mean, cov) in enumerate(zip(means, covariances)):\n",
    "        # Generate points for each cluster with given mean and covariance\n",
    "        cluster_points = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "        X.append(cluster_points)\n",
    "        y.extend([i] * n_samples)\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Define means and covariances for diagonal elliptical clusters\n",
    "means = [[0, 0], [-3, 3], [0, 5], [2, 2]]\n",
    "covariances = [\n",
    "    [[1, 0.5], [0.5, 1]],  # Slightly rotated ellipse\n",
    "    [[0.3, 0.2], [0.2, 1.2]],  # Narrow ellipse\n",
    "    [[1.5, -0.7], [-0.7, 0.5]],  # Wider, tilted ellipse\n",
    "    [[0.5, -0.3], [-0.3, 0.5]],  # Smaller ellipse\n",
    "]\n",
    "\n",
    "# Generate custom diagonal blobs\n",
    "X, y_true = generate_diagonal_blobs(means, covariances, 100)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=40)\n",
    "plt.title(\"Generated Data with Diagonal Elliptical Clusters\")\n",
    "plt.show()\n",
    "\n",
    "# Function to draw ellipses based on covariance matrices\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    ax = ax or plt.gca()\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle=angle, **kwargs))\n",
    "\n",
    "# Plot GMM with elliptical boundaries\n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='plasma', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)\n",
    "\n",
    "# Apply Gaussian Mixture Model to generated data\n",
    "gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
    "plot_gmm(gmm, X)\n",
    "plt.title(\"GMM with Diagonal Elliptical Cluster Boundaries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbfa4c",
   "metadata": {},
   "source": [
    "Points near the cluster boundaries have lower certainty, reflected in smaller marker sizes.\n",
    "\n",
    "2. **Flexible Cluster Shapes**: GMM can model elliptical clusters, with diffent standard deviations, unlike k-Means, which assumes spherical clusters with uniform cluster sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b37a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the blobs\n",
    "n_samples = [400, 100, 100]  \n",
    "centers = [(0, 0), (4, 4), (-4, 4)]  # Center coordinates\n",
    "cluster_std = [2., 0.5, 0.5]  # Standard deviations for each blob\n",
    "\n",
    "# Generate blobs\n",
    "X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=1)\n",
    "\n",
    "# Plot GMM with elliptical boundaries\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)\n",
    "plot_gmm(gmm, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4bf02",
   "metadata": {},
   "source": [
    "GMM is able to model more complex, elliptical cluster boundaries, addressing one of the main limitations of k-Means.\n",
    "\n",
    "## Expectation-Maximization (EM) Algorithm\n",
    "The **Expectation-Maximization (EM)** algorithm is a statistical technique used for finding maximum likelihood estimates in models with latent variables, such as GMMs. The EM algorithm consists of two main steps:\n",
    "\n",
    "- **Expectation (E) Step**: This step calculates the expected value of the latent variables given the current parameter estimates and the data.\n",
    "- **Maximization (M) Step**: In this step, the parameters are updated by maximizing the expected likelihood found in the E step.\n",
    "\n",
    "The E and M steps are repeated until the algorithm converges, usually when the change in the log-likelihood is below a certain threshold.\n",
    "\n",
    "\n",
    "## Comparison between k-Means and GMM\n",
    "\n",
    "The following table highlights the similarities and differences between the k-Means and GMM algorithms in terms of their iteration steps:\n",
    "``````{list-table}\n",
    ":header-rows: 1\n",
    ":widths: 5 20 20\n",
    "\n",
    "* - Step\n",
    "  - $k$-Means\n",
    "  - GMM\n",
    "* - **Init**\n",
    "  - Select $K$ cluster centers $(\\mathbf{m}_1^{(1)}, \\ldots, \\mathbf{m}_K^{(1)})$\n",
    "  - $K$ components with means $\\mu_k$, covariance $\\Sigma_k$, and mixing coefficients $P_k$\n",
    "* - **E:**\n",
    "  - Allocate data points to clusters: \n",
    "  -  Update the probability that component $k$ generated data point $\\mathbf{x}_n$:\n",
    "* -\n",
    "  - ```{math}\n",
    "    S_i^{(t)} = \\{\\mathbf{x}_p: ||\\mathbf{x}_p - \\mathbf{m}_i^{(t)}||^2 \\le ||\\mathbf{x}_p - \\mathbf{m}_j^{(t)}||^2 \\; \\forall j \\}\n",
    "    ```\n",
    "  - ```{math}\n",
    "    \\gamma_{nk} = \\frac{P_k \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K P_j \\mathcal{N}(\\mathbf{x}_n | \\mu_j, \\Sigma_j)} \n",
    "    ```\n",
    "* - **M:**\n",
    "  - Re-estimate cluster centers: \n",
    "  - Calculate estimated number of cluster members $N_k$, means $\\mu_k$, covariance $\\Sigma_k$, and mixing coefficients $P_k$:\n",
    "* - \n",
    "  - ```{math}\n",
    "    \\mathbf{m}_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum_{\\mathbf{x}_j \\in S_i^{(t)}} \\mathbf{x}_j \n",
    "    ```\n",
    "  - ```{math}\n",
    "    N_k = \\sum_{n=1}^N \\gamma_{nk}, \\\\\n",
    "    \\mu_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk} \\mathbf{x}_n, \\\\\n",
    "    \\Sigma_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk} (\\mathbf{x}_n - \\mu_k^{\\text{new}})(\\mathbf{x}_n - \\mu_k^{\\text{new}})^T, \\\\\n",
    "    P_k^{\\text{new}} = \\frac{N_k}{N}\n",
    "    ```\n",
    "* - **Stop:**\n",
    "  - Stop when there are no changes in cluster assignments.\n",
    "  - Stop when the log-likelihood does not increase\n",
    "* -\n",
    "  -\n",
    "  - ```{math}\n",
    "    \\ln \\Pr(\\mathbf{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\mathbf{P}) = \\sum_{n=1}^N \\ln \\left( \\sum_{k=1}^K P_k \\mathcal{N}(\\mathbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right)\n",
    "    ```\n",
    "``````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
